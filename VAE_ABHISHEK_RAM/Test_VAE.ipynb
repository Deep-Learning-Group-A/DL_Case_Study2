{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)  # Input: RGB\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten for linear layers\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 256 * 4 * 4)\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1)  # Output: RGB\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.fc(z)\n",
    "        z = z.view(z.size(0), 256, 4, 4)  # Reshape for deconvolutions\n",
    "        z = F.relu(self.deconv1(z))\n",
    "        z = F.relu(self.deconv2(z))\n",
    "        z = F.relu(self.deconv3(z))\n",
    "        recon_x = torch.sigmoid(self.deconv4(z))  # Scale to [0, 1]\n",
    "        return recon_x\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        z = mu + std * torch.randn_like(std)  # Reparameterization trick\n",
    "        recon_x = self.decoder(z)\n",
    "        return recon_x, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function combining reconstruction loss and KL divergence\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # Reconstruction loss (BCE or MSE depending on dataset)\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')  # Use MSE for continuous images\n",
    "    # KL divergence loss\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "def train_vae(vae, dataloader, epochs, latent_dim, device):\n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "    vae = vae.to(device)\n",
    "    vae.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            recon_images, mu, logvar = vae(images)\n",
    "            loss = loss_function(recon_images, images, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader.dataset):.4f}\")\n",
    "\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# Updated device setting for CPU\n",
    "device = torch.device(\"cpu\")  # Set to 'cpu'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomHorizontalFlip(),  # Augmentation: Horizontal flipping\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=\"photos_classes\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images for evaluation\n",
    "def generate_and_save_images(vae, num_samples, latent_dim, device):\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        generated_images = vae.decoder(z).cpu().detach()\n",
    "    return generated_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display generated images\n",
    "def display_last_generated_images(images, num_last=5):\n",
    "    if images.max() > 1:\n",
    "        images = images / 255.0  # Normalize for display\n",
    "    images = images.cpu().detach()\n",
    "\n",
    "    if images.shape[1] == 1:  # If grayscale, repeat to create RGB\n",
    "        images = images.repeat(1, 3, 1, 1)\n",
    "\n",
    "    images = images[-num_last:]  # Get the last N images\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(1, num_last, i + 1)\n",
    "        plt.imshow(image.permute(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 8716.5284\n",
      "Epoch 2/50, Loss: 7814.0405\n",
      "Epoch 3/50, Loss: 7439.6109\n",
      "Epoch 4/50, Loss: 7194.4680\n",
      "Epoch 5/50, Loss: 7029.1695\n",
      "Epoch 6/50, Loss: 6922.0646\n",
      "Epoch 7/50, Loss: 6823.6672\n",
      "Epoch 8/50, Loss: 6764.7547\n",
      "Epoch 9/50, Loss: 6713.7690\n",
      "Epoch 10/50, Loss: 6682.7555\n",
      "Epoch 11/50, Loss: 6652.4398\n",
      "Epoch 12/50, Loss: 6629.7009\n",
      "Epoch 13/50, Loss: 6608.1597\n",
      "Epoch 14/50, Loss: 6594.4859\n",
      "Epoch 15/50, Loss: 6580.2053\n",
      "Epoch 16/50, Loss: 6570.9692\n",
      "Epoch 17/50, Loss: 6562.7112\n",
      "Epoch 18/50, Loss: 6554.2113\n",
      "Epoch 19/50, Loss: 6543.2068\n",
      "Epoch 20/50, Loss: 6537.0615\n",
      "Epoch 21/50, Loss: 6530.1085\n",
      "Epoch 22/50, Loss: 6523.2698\n",
      "Epoch 23/50, Loss: 6518.3813\n",
      "Epoch 24/50, Loss: 6512.0023\n",
      "Epoch 25/50, Loss: 6507.5854\n",
      "Epoch 26/50, Loss: 6501.2923\n",
      "Epoch 27/50, Loss: 6497.5254\n",
      "Epoch 28/50, Loss: 6489.4555\n",
      "Epoch 29/50, Loss: 6486.8823\n",
      "Epoch 30/50, Loss: 6482.3759\n",
      "Epoch 31/50, Loss: 6475.8285\n",
      "Epoch 32/50, Loss: 6472.8637\n",
      "Epoch 33/50, Loss: 6467.2269\n",
      "Epoch 34/50, Loss: 6461.4004\n",
      "Epoch 35/50, Loss: 6459.7872\n",
      "Epoch 36/50, Loss: 6455.5706\n",
      "Epoch 37/50, Loss: 6450.9528\n",
      "Epoch 38/50, Loss: 6445.9563\n",
      "Epoch 39/50, Loss: 6445.3037\n",
      "Epoch 40/50, Loss: 6440.1686\n",
      "Epoch 41/50, Loss: 6435.8021\n",
      "Epoch 42/50, Loss: 6432.8033\n",
      "Epoch 43/50, Loss: 6429.0053\n",
      "Epoch 44/50, Loss: 6426.4087\n",
      "Epoch 45/50, Loss: 6423.3207\n",
      "Epoch 46/50, Loss: 6419.4323\n",
      "Epoch 47/50, Loss: 6415.7166\n",
      "Epoch 48/50, Loss: 6415.7544\n",
      "Epoch 49/50, Loss: 6410.3594\n",
      "Epoch 50/50, Loss: 6408.5346\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Generate and display the last 5 images\u001b[39;00m\n\u001b[0;32m     12\u001b[0m generated_images \u001b[38;5;241m=\u001b[39m generate_and_save_images(vae, num_samples\u001b[38;5;241m=\u001b[39mnum_samples, latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mdisplay_last_generated_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m, in \u001b[0;36mdisplay_last_generated_images\u001b[1;34m(images, num_last)\u001b[0m\n\u001b[0;32m      8\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m images \u001b[38;5;241m=\u001b[39m images[\u001b[38;5;241m-\u001b[39mnum_last:]  \u001b[38;5;66;03m# Get the last N images\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(images):\n\u001b[0;32m     13\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, num_last, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Main training and evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    latent_dim = 64\n",
    "    epochs = 200\n",
    "    num_samples = 500\n",
    "\n",
    "    # Initialize and train VAE\n",
    "    vae = VAE(latent_dim=latent_dim)\n",
    "    vae = train_vae(vae, dataloader, epochs=epochs, latent_dim=latent_dim, device=device)\n",
    "\n",
    "    # Generate and display the last 5 images\n",
    "    generated_images = generate_and_save_images(vae, num_samples=num_samples, latent_dim=latent_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_last_generated_images(generated_images, num_last=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import inception_v3\n",
    "from torchvision.transforms import Resize, Normalize\n",
    "from scipy.linalg import sqrtm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Load Pretrained InceptionV3\n",
    "def load_inception_model():\n",
    "    model = inception_v3(pretrained=True, transform_input=False)\n",
    "    model.fc = torch.nn.Identity()  # Remove the classification layer\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# Compute Inception Score (IS)\n",
    "def compute_inception_score(images, inception, device, splits=10):\n",
    "    \"\"\"\n",
    "    Compute the Inception Score for generated images.\n",
    "\n",
    "    Args:\n",
    "        images (torch.Tensor): Tensor of generated images, shape [N, 3, H, W].\n",
    "        inception (torch.nn.Module): Pretrained InceptionV3 model.\n",
    "        device (torch.device): Device to run the model.\n",
    "        splits (int): Number of splits for score calculation.\n",
    "\n",
    "    Returns:\n",
    "        (float, float): Mean and standard deviation of Inception Score.\n",
    "    \"\"\"\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = F.softmax(inception(images), dim=1).cpu().numpy()  # Predict class probabilities\n",
    "    split_scores = []\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (len(preds) // splits): (k + 1) * (len(preds) // splits)]\n",
    "        kl_div = part * (np.log(part) - np.log(np.mean(part, axis=0, keepdims=True)))\n",
    "        split_scores.append(np.exp(np.mean(np.sum(kl_div, axis=1))))\n",
    "    return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "\n",
    "# Compute Frechet Inception Distance (FID)\n",
    "def compute_fid(real_activations, fake_activations):\n",
    "    \"\"\"\n",
    "    Compute the Frechet Inception Distance (FID) between real and generated images.\n",
    "\n",
    "    Args:\n",
    "        real_activations (np.ndarray): Activations of real images.\n",
    "        fake_activations (np.ndarray): Activations of generated images.\n",
    "\n",
    "    Returns:\n",
    "        float: FID score.\n",
    "    \"\"\"\n",
    "    mu1, sigma1 = np.mean(real_activations, axis=0), np.cov(real_activations, rowvar=False)\n",
    "    mu2, sigma2 = np.mean(fake_activations, axis=0), np.cov(fake_activations, rowvar=False)\n",
    "    diff = mu1 - mu2\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = np.sum(diff**2) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return fid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor\n",
    "\n",
    "# Preprocess images for InceptionV3\n",
    "def preprocess_images_for_inception(images, resize=299):\n",
    "    transform = Compose([\n",
    "        Resize((resize, resize)),  # Resize to 299x299 for InceptionV3\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Inception normalization\n",
    "    ])\n",
    "    images = images.float() if images.dtype == torch.uint8 else images\n",
    "    images = images / 255.0 if images.max() > 1 else images  # Scale to [0, 1] if needed\n",
    "    return torch.stack([transform(image) for image in images])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(vae, dataloader, num_generated, latent_dim, device):\n",
    "    \"\"\"\n",
    "    Evaluate Inception Score (IS) and FID for a trained VAE.\n",
    "\n",
    "    Args:\n",
    "        vae (torch.nn.Module): Trained VAE model.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader for real images.\n",
    "        num_generated (int): Number of generated images for evaluation.\n",
    "        latent_dim (int): Dimension of the latent space.\n",
    "        device (torch.device): Device to run evaluation.\n",
    "\n",
    "    Returns:\n",
    "        (float, float, float): Inception Score (mean, std) and FID score.\n",
    "    \"\"\"\n",
    "    # Load pretrained InceptionV3\n",
    "    inception = load_inception_model().to(device)\n",
    "\n",
    "    # Generate images\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_generated, latent_dim).to(device)\n",
    "        generated_images = vae.decoder(z).cpu().detach()\n",
    "\n",
    "    # Preprocess generated images\n",
    "    generated_images = preprocess_images_for_inception(generated_images).to(device)\n",
    "\n",
    "    # Compute activations for generated images\n",
    "    with torch.no_grad():\n",
    "        fake_activations = inception(generated_images).cpu().numpy()\n",
    "\n",
    "    # Compute activations for real images\n",
    "    real_activations = []\n",
    "    for real_images, _ in dataloader:\n",
    "        real_images = preprocess_images_for_inception(real_images).to(device)\n",
    "        with torch.no_grad():\n",
    "            real_activations.append(inception(real_images).cpu().numpy())\n",
    "    real_activations = np.concatenate(real_activations, axis=0)\n",
    "\n",
    "    # Compute Inception Score\n",
    "    inception_score, inception_std = compute_inception_score(generated_images, inception, device)\n",
    "\n",
    "    # Compute FID\n",
    "    fid = compute_fid(real_activations, fake_activations)\n",
    "\n",
    "    print(f\"Inception Score: {inception_score} ± {inception_std}\")\n",
    "    print(f\"Frechet Inception Distance: {fid}\")\n",
    "    return inception_score, inception_std, fid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate metrics\n",
    "latent_dim = 64\n",
    "num_generated = 500\n",
    "\n",
    "# Ensure the DataLoader has the real dataset\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Evaluate Inception Score and FID\n",
    "is_mean, is_std, fid = evaluate_metrics(vae, dataloader, num_generated, latent_dim, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'D:\\Loyalist Subjects\\Semester 3\\2024F-T3 AISC2007 - Deep Learning 01\\Deep Learning\\yelp_photos\\photos'\n",
    "json_file = 'D:\\Loyalist Subjects\\Semester 3\\2024F-T3 AISC2007 - Deep Learning 01\\Deep Learning\\yelp_photos\\photos.json'\n",
    "label_filter = 'drink'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m----> 2\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[43mimage_size\u001b[49m),\n\u001b[0;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mCenterCrop(image_size),\n\u001b[0;32m      4\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m      5\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize([\u001b[38;5;241m0.5\u001b[39m] \u001b[38;5;241m*\u001b[39m channels_img, [\u001b[38;5;241m0.5\u001b[39m] \u001b[38;5;241m*\u001b[39m channels_img),\n\u001b[0;32m      6\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_size' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python10001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
